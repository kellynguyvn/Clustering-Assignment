{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering  of documents"
      ],
      "metadata": {
        "id": "lEffk-joF8tD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Collection"
      ],
      "metadata": {
        "id": "pV7kMnvyGPHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUSgpx2wKTyT",
        "outputId": "3d21a21b-b7a0-4ce0-b638-34508d3473e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/239.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.5.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjIwoZnoF62M",
        "outputId": "71514b00-787f-4e44-8202-6a14e06b265f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from docx import Document\n",
        "\n",
        "def read_docx(file_path):\n",
        "    \"\"\" Read a .docx file and return its text content. \"\"\"\n",
        "    doc = Document(file_path)\n",
        "    return \" \".join([para.text for para in doc.paragraphs])\n",
        "\n",
        "# Paths to the documents\n",
        "doc1_path = '/content/Doc 1.docx'\n",
        "doc2_path = '/content/Doc 2.docx'\n",
        "\n",
        "# Reading the documents\n",
        "doc1_content = read_docx(doc1_path)\n",
        "doc2_content = read_docx(doc2_path)\n",
        "\n",
        "len(doc1_content), len(doc2_content)  # Returning the length to check successful reading\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "Zw48jqS_GPiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from docx import Document\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "785NDNspGDG6",
        "outputId": "d47c0109-9708-4c0e-fe56-3d4750a54b68"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_docx(file_path):\n",
        "    \"\"\" Read a .docx file and return its text content. \"\"\"\n",
        "    doc = Document(file_path)\n",
        "    return \" \".join([para.text for para in doc.paragraphs])\n",
        "\n",
        "# Paths to the documents\n",
        "doc1_path = '/content/Doc 1.docx'\n",
        "doc2_path = '/content/Doc 2.docx'\n",
        "\n",
        "# Reading the documents\n",
        "doc1_content = read_docx(doc1_path)\n",
        "doc2_content = read_docx(doc2_path)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Remove non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return ' '.join(tokens)\n"
      ],
      "metadata": {
        "id": "MZNYWy8sKxTS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of document contents\n",
        "documents = [doc1_content, doc2_content]\n",
        "\n",
        "# Preprocess documents\n",
        "processed_documents = [preprocess_text(doc) for doc in documents]"
      ],
      "metadata": {
        "id": "JtncB-YALBIP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Generation"
      ],
      "metadata": {
        "id": "wr5sLKzyGQAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Example: Using BERT model for embeddings\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def get_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    outputs = model(**inputs)\n",
        "    # Take the mean across the sequence length dimension to get a single vector per input document\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
        "\n",
        "# Generate embeddings for each document\n",
        "embeddings = [get_embedding(doc) for doc in processed_documents]\n",
        "\n",
        "# Checking the shape of the first embedding to ensure it's 1D\n",
        "print(embeddings[0].shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHkZ9qG5GE-n",
        "outputId": "b2f5c169-35e7-4825-cb5a-f3ac9faec319"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(768,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering"
      ],
      "metadata": {
        "id": "-nTEJwcPGQh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Define the number of clusters\n",
        "n_clusters = 2  # Adjust based on your data and needs\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=n_clusters)\n",
        "clusters = kmeans.fit_predict(embeddings)\n",
        "\n",
        "# Output the cluster assignment for each document\n",
        "print(clusters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S_pOPTaGGt4",
        "outputId": "f66081ee-1cf5-40c1-ab6a-d84159672d3f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "nXGXf-pPGQ4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM embeddings can be large and resource-intensive. Ensure that your environment has sufficient computational resources.\n",
        "The choice of LLM (e.g., BERT, GPT) can be adjusted based on your specific needs and the nature of your documents.\n",
        "This code provides a basic framework. Depending on your data and requirements, further customization may be needed."
      ],
      "metadata": {
        "id": "7ub0c9h4GKf9"
      }
    }
  ]
}